{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k3larra/IKR/blob/master/ikr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hTdz9j-fzHH"
      },
      "source": [
        "# \"When Can I Trust It?\" Contextualising Explainability Methods for Classifiers\n",
        "In experiment 3 for the paper with the above title we compare internal knowledge representations for a number of, on ImageNet-1k pretrained models. For the comparison the model agnostic XAI method Occlusion is used. By doing this we can, from some perspective, compare internal representations for the models. \n",
        "\n",
        "We can then draw the conclusion that the neural networks puts emphasis on different areas in the images. This is not any surprise but it poses questions when we are to select models for our explanations, what model is most trustworthy, and from what perspective. \n",
        "\n",
        "Analogous in a human setting, how to chose the best expert?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Iwwa9AA2j9Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "_-NgG2iio2h6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ov94kyvGvx0"
      },
      "source": [
        "# Test set, models and label "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testset\n",
        "%%capture\n",
        "from zipfile import ZipFile\n",
        "! git clone https://github.com/k3larra/IKR\n",
        "with ZipFile('/content/IKR/testset/testset.zip', 'r') as archive:\n",
        "  archive.extractall('/content/testset')"
      ],
      "metadata": {
        "id": "KUjGw1CTbjNL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ImageNet1k labels \n",
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "num_classes = len(categories)\n",
        "\n",
        "def label_to_idx(label):\n",
        "  return categories.index(label)\n",
        "\n",
        "def idx_to_label(idx):\n",
        "  return categories[idx]"
      ],
      "metadata": {
        "id": "iQYA_CTPxEFi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load models\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from torchvision.models import resnet101, ResNet101_Weights\n",
        "model_resnet101 = resnet101(weights=ResNet101_Weights.IMAGENET1K_V2)\n",
        "model_resnet101.eval()\n",
        "model_resnet101.name = \"ResNet101\"\n",
        "model_resnet101 = model_resnet101.to(device)\n",
        "\n",
        "from torchvision.models import resnet152, ResNet152_Weights\n",
        "model_resnet152 = resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
        "model_resnet152.eval()\n",
        "model_resnet152.name = \"ResNet152\"\n",
        "model_resnet152 = model_resnet152.to(device)\n",
        "\n",
        "from torchvision.models import googlenet, GoogLeNet_Weights\n",
        "model_googlenet = googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "model_googlenet.eval()\n",
        "model_googlenet.name = \"GoogLeNet\"\n",
        "model_googlenet = model_googlenet.to(device)\n",
        "\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "model_inception_v3 = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
        "model_inception_v3.eval()\n",
        "model_inception_v3.name = \"Inception_V3\"\n",
        "model_inception_v3 = model_inception_v3.to(device)\n",
        "\n",
        "from torchvision.models import efficientnet_v2_s,EfficientNet_V2_S_Weights\n",
        "model_efficientnet_v2_s = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
        "model_efficientnet_v2_s.eval()\n",
        "model_efficientnet_v2_s.name = \"Efficientnet_V2_s\"\n",
        "model_efficientnet_v2_s = model_efficientnet_v2_s.to(device)\n",
        "\n",
        "from torchvision.models import regnet_y_8gf,RegNet_Y_8GF_Weights\n",
        "model_regnet_y_8gf = regnet_y_8gf(weights=RegNet_Y_8GF_Weights.IMAGENET1K_V2)\n",
        "model_regnet_y_8gf.eval()\n",
        "model_regnet_y_8gf.name = \"RegNet_Y_8GF\"\n",
        "model_regnet_y_8gf = model_regnet_y_8gf.to(device)\n",
        "\n",
        "from torchvision.models import swin_t,Swin_T_Weights\n",
        "model_swin_t = swin_t(weights=Swin_T_Weights.IMAGENET1K_V1)\n",
        "model_swin_t.eval()\n",
        "model_swin_t.name = \"Swin_T_Weights\"\n",
        "model_swin_t = model_swin_t.to(device)\n",
        "\n",
        "from torchvision.models import convnext_tiny,ConvNeXt_Tiny_Weights\n",
        "model_convnext_tiny = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "model_convnext_tiny.eval()\n",
        "model_convnext_tiny.name = \"ConvNeXt_Tiny\"\n",
        "model_convnext_tiny = model_convnext_tiny.to(device)\n",
        "\n",
        "experiment_models = [model_resnet101,\n",
        "                     model_resnet152,\n",
        "                     model_googlenet,\n",
        "                     model_inception_v3,\n",
        "                     model_efficientnet_v2_s,\n",
        "                     model_regnet_y_8gf,\n",
        "                     model_swin_t,\n",
        "                     model_convnext_tiny]\n",
        "experiment_weights= [ResNet101_Weights.IMAGENET1K_V2,\n",
        "                     ResNet152_Weights.IMAGENET1K_V2,\n",
        "                     GoogLeNet_Weights.IMAGENET1K_V1,\n",
        "                     Inception_V3_Weights.IMAGENET1K_V1,\n",
        "                     EfficientNet_V2_S_Weights.IMAGENET1K_V1,\n",
        "                     RegNet_Y_8GF_Weights.IMAGENET1K_V2,\n",
        "                     Swin_T_Weights.IMAGENET1K_V1,\n",
        "                     ConvNeXt_Tiny_Weights.IMAGENET1K_V1]"
      ],
      "metadata": {
        "id": "uDjm8pD6qaeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data transformation and inference\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "eval_transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "transform_normalize = transforms.Normalize( \n",
        "     mean=[0.485, 0.456, 0.406],\n",
        "     std=[0.229, 0.224, 0.225]\n",
        " )\n",
        "\n",
        "def transform_eval_data(img_path, eval_transform = None):\n",
        "  image = Image.open(img_path).convert('RGB')\n",
        "  if eval_transform:\n",
        "      image = eval_transform(image)\n",
        "      image = transform_normalize(image) \n",
        "  image = image.float()\n",
        "  return image\n",
        "\n",
        "def norm_image(image):\n",
        "  data_min = np.min(image, axis=(1,2), keepdims=True)\n",
        "  data_max = np.max(image, axis=(1,2), keepdims=True)\n",
        "  image = (image - data_min) / (data_max - data_min)\n",
        "  return image\n",
        "\n",
        "def show_image(image, title):\n",
        "    \"\"\"Show image with landmarks\"\"\"\n",
        "    plt.imshow(image)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"off\")\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "def eval_model(experiment_set, test_model, print_eval=False):\n",
        "  test_model.eval()   # Set model to evaluate mode\n",
        "  for experiment_sample in experiment_set:\n",
        "    with torch.no_grad():\n",
        "      input_img = experiment_sample.unsqueeze(0).to(device)\n",
        "      prediction = test_model(input_img).squeeze(0).softmax(0)\n",
        "      values,indices=prediction.topk(5)\n",
        "      if print_eval:\n",
        "        show_image(norm_image(input_img.squeeze().permute(1,2,0).cpu().numpy()), test_model.name)\n",
        "        for i in range(0,5):\n",
        "          print(\"Label\",indices[i].item(),\": class \",  idx_to_label(indices[i]),\" probability :\", str(int(np.round(values[i].item(),3)*100)),\"%\")\n",
        "\n",
        "def get_all_files(experiment_path):\n",
        "  loaded_files = []\n",
        "  for f in sorted(os.listdir(experiment_path)):\n",
        "    if f.endswith('.PNG') or f.endswith('.png') or f.endswith('.jpg') or f.endswith('.JPG'):\n",
        "      loaded_files.append(f)\n",
        "  return loaded_files\n",
        "\n",
        "def load_experiment_data(experiment_path, test_model, weights, use_predifined_tranformation=False, plot_data=False, evaluate_model=False, print_evaluation=False):\n",
        "  experiment_set = []\n",
        "  eval_dir = get_all_files(experiment_path)\n",
        "  eval_size = len(eval_dir)\n",
        "  for i in range(eval_size):\n",
        "    if use_predifined_tranformation:\n",
        "      transforms = weights.transforms()\n",
        "      experiment_set.append(transforms(read_image(experiment_path + eval_dir[i])))\n",
        "    else:\n",
        "      experiment_set.append(transform_eval_data(experiment_path + eval_dir[i],eval_transform))\n",
        "  if plot_data:\n",
        "    figure = plt.figure(figsize=(8, 8))\n",
        "    cols, rows = 3, 1\n",
        "    for i in range(1, cols * rows + 1):\n",
        "        img = experiment_set[i-1]\n",
        "        figure.add_subplot(rows, cols, i)\n",
        "        plt.title(img.shape)\n",
        "        plt.axis(\"off\")\n",
        "        plt.imshow(norm_image(img.permute(1,2,0).numpy()))\n",
        "    plt.show()\n",
        "  if evaluate_model:\n",
        "    eval_model(experiment_set, test_model, print_evaluation)\n",
        "  return experiment_set"
      ],
      "metadata": {
        "id": "ks7bGMU3RUxY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ1QaOGH3bdz"
      },
      "source": [
        "# XAI Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Occlusion\n",
        "Using the implementation from [captum.ai](captum.ai)"
      ],
      "metadata": {
        "id": "jSoyCZkgES4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install captum \n",
        "from captum.attr import Occlusion\n",
        "from captum.attr import LayerAttribution\n",
        "from captum.attr import visualization as viz\n",
        "def calculate_occlusion(experiment_model, target_idx, input_img, top_candidates, top_probs, save_path=\"\"):\n",
        "  #experiment_model = experiment_model.to(device)\n",
        "  input_img = input_img.to(device)\n",
        "  occlusion = Occlusion(experiment_model)\n",
        "  attributions = occlusion.attribute(input_img,\n",
        "                                    sliding_window_shapes=(1, 32, 32),\n",
        "                                    strides=(1, 32, 32),\n",
        "                                    target=target_idx,\n",
        "                                    baselines = 0)\n",
        "  input_img = input_img.squeeze()\n",
        "  result = viz.visualize_image_attr(attributions[0].cpu().permute(1,2,0).detach().numpy(),\n",
        "                              input_img.cpu().permute(1,2,0).detach().numpy(), \n",
        "                              method=\"blended_heat_map\",\n",
        "                              sign=\"all\",\n",
        "                              fig_size=(6,6))\n",
        "  prob=0\n",
        "  for idx, candate in enumerate(top_candidates): #Find acc for target\n",
        "    if candate.item()== target_idx:\n",
        "      prob=str(int(np.round(top_probs[idx].item(),2)*100))\n",
        "  print(\"Top candidate:\",str(idx_to_label(top_candidates[0].item())), \"with prob:\",str(int(np.round(top_probs[0].item(),2)*100)),\" model:\",experiment_model.name)    \n",
        "  print(\"occlusion with target: \",str(idx_to_label(target_idx)), \"prob: \", prob,\"%\")\n",
        "  result[0].savefig(save_path,bbox_inches='tight', pad_inches = 0)\n",
        "  return attributions"
      ],
      "metadata": {
        "id": "9brXL4IfFVbi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "k-EhfqafHnsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(experiment_models, experiment_weights, experiment_path, experiment_name, use_predifined_tranformation=False, save_path=\"\", debug=False):\n",
        "  \"\"\"Saves original and transformed images and makes predictions using models in experiment_models.\n",
        "    Also builds a JSON file containing class probabilities for the 10 clsses with highest prediction score.  \n",
        "    Parameters\n",
        "    --------\n",
        "    target_level: gives ...\n",
        "    \"\"\"\n",
        "  path_to_experiment = save_path + experiment_name.replace(\" \", \"\") + \"/\"\n",
        "  print(\"Experiment name: \",experiment_name)\n",
        "  print(\"Save Path: \", path_to_experiment)\n",
        "  print(\"------------------\")\n",
        "  images=get_all_files(experiment_path)\n",
        "  experiment_set = load_experiment_data(experiment_path,  experiment_models[0], experiment_weights[0],use_predifined_tranformation=False, plot_data=False, evaluate_model = False, print_evaluation=False) \n",
        "  experiment_json_data={}\n",
        "  for idx, experiment_sample in enumerate(experiment_set):\n",
        "      path_to_save = path_to_experiment+\"image\"+str(idx) + \"/\"\n",
        "      if not os.path.exists(path_to_save):\n",
        "        os.makedirs(path_to_save)\n",
        "      image_info={}    \n",
        "      image_path_transformed = path_to_save+\"imagetransformed\"+str(idx)+\".PNG\"\n",
        "      image_path = path_to_save+\"image\"+str(idx)+\".PNG\"\n",
        "      image = Image.open(experiment_path+images[idx])\n",
        "      image.save(image_path)\n",
        "      image.close\n",
        "      im = norm_image(experiment_sample.cpu().permute(1,2,0).detach().numpy())\n",
        "      plt.imsave(image_path_transformed,im)\n",
        "      image_info[\"image_path\"] = image_path\n",
        "      image_info[\"image_path_transformed\"] = image_path_transformed\n",
        "      nbr_maps=len(experiment_models)\n",
        "      saliency_map_aggregate=torch.zeros(1000,nbr_maps,147) #Holds the 1k Imagenet label_idx for five alternatives for the modes and 7x7x3 images flattended\n",
        "      for model_index,model in enumerate(experiment_models):\n",
        "        model_prediction = {}\n",
        "        ##Here we need to change transform!!!!!\n",
        "        experiment_sample = experiment_set[idx]\n",
        "        experiment_sample = experiment_sample.unsqueeze(0)\n",
        "        top_candidates, top_probs, jdata = process_input(experiment_sample, model, debug=debug) \n",
        "        image_info[model.name]=jdata\n",
        "        saliency_json = create_saliency_maps(model,experiment_sample,top_candidates,top_probs,path_to_save,saliency_map_aggregate,model_index,nbr_maps=nbr_maps,debug=debug)\n",
        "        image_info[model.name][\"xai\"]=saliency_json\n",
        "      image_info[\"diff_mean_maps\"] = calc_show_difference(saliency_map_aggregate,experiment_path,path_to_save,idx,debug=debug)\n",
        "      experiment_json_data[idx]=image_info\n",
        "      #check\n",
        "      experiment_data = json.dumps(experiment_json_data[idx]) \n",
        "      with open(path_to_experiment + 'structure'+str(idx)+'.json', 'w') as outfile:\n",
        "        outfile.write(experiment_data)\n",
        "  if debug:\n",
        "    print(json.dumps(experiment_json_data, indent=2))    \n",
        "  experiment_data = json.dumps(experiment_json_data) \n",
        "  with open(path_to_experiment + 'structure.json', 'w') as outfile:\n",
        "      outfile.write(experiment_data)\n",
        "\n",
        "def process_input(input_img, experiment_model, debug=False): #This should be identival to above......\n",
        "  input_img = input_img.to(device)\n",
        "  experiment_model = experiment_model.to(device)\n",
        "  output = experiment_model(input_img)\n",
        "  probabilities = F.softmax(output[0], dim=0)\n",
        "  top_prob, top_catid = torch.topk(probabilities, num_classes)\n",
        "  jsonData={}\n",
        "  for i in range(10):\n",
        "      prediction={}\n",
        "      prob=top_prob[i].item()\n",
        "      prediction[\"probability\"] = np.round(top_prob[i].item(),9)\n",
        "      prediction[\"label\"] = idx_to_label(top_catid[i])\n",
        "      prediction[\"labelid\"] = top_catid[i].item()\n",
        "      jsonData[i]=prediction\n",
        "  if debug:\n",
        "    print(jsonData)\n",
        "  return top_catid, top_prob, jsonData\n",
        "\n",
        "def create_saliency_maps(experiment_model,experiment_sample,top_candidates,top_probs,path_to_save,saliency_map_aggregate,model_index,nbr_maps=5,debug=False):\n",
        "    \"\"\" Creates saliency maps for nbr_maps with highest class probability\n",
        "    Parameters\n",
        "    -------- \n",
        "    nbr_maps: the number of top_candidates saliency maps are created for\n",
        "    \"\"\"\n",
        "    path_to_save = path_to_save+experiment_model.name+\"/occlusion/\"\n",
        "    if not os.path.exists(path_to_save):\n",
        "      os.makedirs(path_to_save)\n",
        "    json_data={}\n",
        "    json_data[\"XAI-method\"] = \"Occlusion\"\n",
        "    json_data[\"image_path\"] = path_to_save\n",
        "    json_data[\"settings\"] = \"sliding_window_shapes=(1, 32, 32),strides=(1, 32, 32)\"\n",
        "    json_data[\"code_ref\"] = \"https://captum.ai/api/occlusion.html\"\n",
        "    for i in range(0,nbr_maps):\n",
        "      #json_data[i] = path_to_save+idx_to_label(top_candidates[i]).replace(\" \", \"_\")+\".PNG\"\n",
        "      json_data[i] = path_to_save+str(top_candidates[i].item())+\".PNG\"\n",
        "      saliency_map =calculate_occlusion(experiment_model,\n",
        "                            target_idx=top_candidates[i],\n",
        "                            input_img=experiment_sample,\n",
        "                            top_candidates=top_candidates, \n",
        "                            top_probs=top_probs,   \n",
        "                            save_path=json_data[i])\n",
        "      json_data[\"metrix_\"+str(i)] = calculate_metrix_for_saliency_map(saliency_map,top_candidates[i])\n",
        "      if debug:\n",
        "       print(\"saliency_map.shape\",saliency_map.shape)\n",
        "       print(json.dumps(json_data, indent=2))\n",
        "      add_saliency_row(saliency_map_aggregate,saliency_map,top_candidates[i],model_index,debug=debug)\n",
        "    return json_data\n",
        "\n",
        "def calculate_metrix_for_saliency_map(saliency_map,target_idx):\n",
        "  json_metrix={}\n",
        "  saliency_map = transforms.functional.resize(saliency_map,(7,7))\n",
        "  saliency_map = torch.mean(saliency_map,1)\n",
        "  saliency_map = torch.flatten(saliency_map)\n",
        "  json_metrix[\"target_idx\"]= str(target_idx.item())\n",
        "  raw_string=\"\"\n",
        "  for i in range(0,49):\n",
        "    if i>0:\n",
        "      raw_string=raw_string+\",\"+str(np.round((saliency_map[i]).item(),3))\n",
        "    else:\n",
        "      raw_string=str(np.round((saliency_map[i]).item(),3))\n",
        "  saliency_map[saliency_map<0]=0\n",
        "  json_metrix[\"mean\"] = np.round(torch.mean(saliency_map).item(),3)\n",
        "  json_metrix[\"max\"] = np.round(torch.max(saliency_map).item(),3)\n",
        "  json_metrix[\"min\"] = np.round(torch.min(saliency_map).item(),3)\n",
        "  mean_string=\"\"\n",
        "  for i in range(1,50):\n",
        "    if i>1:\n",
        "      mean_string=mean_string+\",\"+str(np.round(torch.mean(torch.topk(saliency_map,i)[0]).item(),3))\n",
        "    else:\n",
        "      mean_string=str(np.round(torch.mean(torch.topk(saliency_map,i)[0]).item(),3))\n",
        "  json_metrix[\"mean_values\"]=mean_string\n",
        "  json_metrix[\"raw_string\"]=raw_string\n",
        "  return json_metrix\n",
        "    \n"
      ],
      "metadata": {
        "id": "ef88U0MJ7v8Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Similarity and dispersion measurement between models."
      ],
      "metadata": {
        "id": "9DGYVMqjE7oN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_saliency_row(saliency_map_aggregate,saliency_map,label_index,model_index,debug=False):\n",
        "  saliency_map = transforms.functional.resize(saliency_map,(7,7))\n",
        "  saliency_map = torch.flatten(saliency_map)\n",
        "  saliency_map=saliency_map[None, :]\n",
        "  saliency_map_aggregate[label_index,model_index]=saliency_map\n",
        "  if debug:\n",
        "    print(\"Adding saliency row with shape \",saliency_map,\" to index:\",label_index,\":\",idx_to_label(int(label_index)))\n",
        "    print(\"Min value is:\",torch.min(saliency_map),\" and max is \",torch.max(saliency_map))\n",
        "  return saliency_map_aggregate\n",
        "\n",
        "def calc_show_difference(saliency_map_aggregate,experiment_path,save_path,image_index,debug=False):\n",
        "  json_data={}\n",
        "  ##Check this needed or not\n",
        "  experiment_set = load_experiment_data(experiment_path,  \"dummy_name\", experiment_weights[0],use_predifined_tranformation=False, plot_data=False, evaluate_model = False, print_evaluation=False) \n",
        "  #Only using this to get the image sp model name not important\n",
        "  experiment_sample = experiment_set[image_index]\n",
        "  input_img = experiment_sample.unsqueeze(0)\n",
        "  input_img = input_img.to(device)\n",
        "  input_img = input_img.squeeze()\n",
        "  for index,value in enumerate(saliency_map_aggregate):\n",
        "    if int(torch.sum(saliency_map_aggregate[index]).item())!=0:\n",
        "      saliency_candidate = torch.zeros(1,147)\n",
        "      for j,row in enumerate(saliency_map_aggregate[index]):\n",
        "        if int(torch.sum(row).item())!=0:\n",
        "          if debug:\n",
        "            print(\"Adding saliency map for\", index,\":\", idx_to_label(index))\n",
        "          if int(torch.sum(saliency_candidate).item())==0:\n",
        "            saliency_candidate=row[None, :]\n",
        "          else:\n",
        "            saliency_candidate = torch.cat((saliency_candidate,row[None, :]),0)\n",
        "      if debug:\n",
        "        print(\"final\",saliency_candidate)\n",
        "        print(\"final\",saliency_candidate.shape)\n",
        "        print(\"saliency_candidate.size(dim=0)\",saliency_candidate.size(dim=0))\n",
        "        print(\"label_index\",index,\":\",idx_to_label(index))\n",
        "      if saliency_candidate.size(dim=0)>1:  \n",
        "        mean_saliency_map = torch.mean(saliency_candidate, 0) #Mean over all columns for all maps\n",
        "        if debug:\n",
        "          print(\"more than one map\")\n",
        "          print(\"saliency_candidate\",saliency_candidate)\n",
        "          print(\"saliency_candidate.shape\",saliency_candidate.shape)\n",
        "          print(\"mean_saliency_map for label_index:\",index,\":\",idx_to_label(int(index)))\n",
        "          print(\"mean_saliency_map\",mean_saliency_map)\n",
        "          print(\"mean_saliency_map shape\",mean_saliency_map.shape)\n",
        "        mean_image=torch.reshape(mean_saliency_map, (1,3,7,7))\n",
        "        attributions = LayerAttribution.interpolate(mean_image, [224,224]) \n",
        "        result=viz.visualize_image_attr(attributions[0].cpu().permute(1,2,0).detach().numpy(),\n",
        "                                    input_img.cpu().permute(1,2,0).detach().numpy(),  \n",
        "                                    method=\"blended_heat_map\",\n",
        "                                    sign=\"positive\",\n",
        "                                    fig_size=(6,6))\n",
        "        print(\"Mean saliency map for prediction \",idx_to_label(index),\":\",index)\n",
        "        save_path_mean = save_path+'mean_image'+str(image_index)+'_candidate'+str(index)+'_occ.PNG'\n",
        "        result[0].savefig(save_path_mean,bbox_inches='tight', pad_inches = 0)\n",
        "        json_data[\"mean_image_path_candidate_\"+str(index)]=save_path_mean\n",
        "        mean_saliency_matrix = transforms.functional.resize(mean_image,(7,7)) #MM and think here of max numbers what to do....perhaps compare to individual for the models.... \n",
        "        if debug:\n",
        "           print(\"mean_saliency_matrix: \",mean_saliency_matrix)\n",
        "           print(\"mean_saliency_matrix.shape: \",mean_saliency_matrix.shape)\n",
        "        mean_saliency_matrix=torch.mean(mean_saliency_matrix,1)\n",
        "        mean_saliency_matrix_flatten = torch.flatten(mean_saliency_matrix)\n",
        "        mean_saliency_matrix_flatten[mean_saliency_matrix_flatten<0]=0 #Remove neg attribs\n",
        "        if debug:\n",
        "           print(\"mean_saliency_matrix: \",mean_saliency_matrix_flatten)\n",
        "           print(\"mean_saliency_matrix.shape: \",mean_saliency_matrix_flatten.shape)\n",
        "        if debug:\n",
        "          print(\"mean saliency map for:\",idx_to_label(index),\" with index:\",index)\n",
        "          print(\"mean:\",np.round(torch.mean(mean_saliency_matrix_flatten).item(),3))\n",
        "          print(\"mean 5:\",np.round(torch.mean(torch.topk(mean_saliency_matrix_flatten,5)[0]).item(),3))\n",
        "          print(\"mean 10:\",np.round(torch.mean(torch.topk(mean_saliency_matrix_flatten,10)[0]).item(),3))\n",
        "        json_data[\"mean_for_candidate_\"+str(index)] = np.round(torch.mean(mean_saliency_matrix_flatten).item(),3)\n",
        "        json_data[\"max_for_candidate_\"+str(index)] = np.round(torch.max(mean_image).item(),3)\n",
        "        json_data[\"min_for_candidate_\"+str(index)] = np.round(torch.min(mean_image).item(),3)\n",
        "        mean_string=\"\"\n",
        "        for i in range(1,50):\n",
        "          if i>1:\n",
        "            mean_string=mean_string+\",\"+str(np.round(torch.mean(torch.topk(mean_saliency_matrix_flatten,i)[0]).item(),3))\n",
        "          else:\n",
        "            mean_string=str(np.round(torch.mean(torch.topk(mean_saliency_matrix_flatten,i)[0]).item(),3))\n",
        "        json_data[\"mean_average_csv_for_candidate_\"+str(index)] = mean_string\n",
        "        std_saliency_map = torch.std(saliency_candidate, 0, unbiased=False) ###Should unbiased be True since it is a sample from different models in reality????\n",
        "        std_image=torch.reshape(std_saliency_map, (1,3,7,7))\n",
        "        attributions = LayerAttribution.interpolate(torch.negative(std_image), [224,224]) #Turn them around\n",
        "        result=viz.visualize_image_attr(attributions[0].cpu().permute(1,2,0).detach().numpy(),\n",
        "                                      input_img.cpu().permute(1,2,0).detach().numpy(),  \n",
        "                                      method=\"blended_heat_map\",\n",
        "                                      sign=\"negative\",\n",
        "                                      fig_size=(6,6))\n",
        "        print(\"Mean standard deviation saliency map for prediction \",idx_to_label(index),\":\",index)\n",
        "        savepath_diff = save_path+'diff_image'+str(image_index)+'_candidate'+str(index)+'_occ.PNG'\n",
        "        result[0].savefig(savepath_diff,bbox_inches='tight', pad_inches = 0)\n",
        "        json_data[\"diff_image_path_candidate_\"+str(index)]=savepath_diff\n",
        "        std_saliency_matrix = transforms.functional.resize(std_image,(7,7)) ##??\n",
        "        if debug:\n",
        "          print(\"std_saliency_matrix: \",std_saliency_matrix)\n",
        "          print(\"std_saliency_matrix.shape: \",std_saliency_matrix.shape)\n",
        "        std_saliency_matrix=torch.mean(std_saliency_matrix,1)\n",
        "        if debug:\n",
        "          print(\"again ? after mean std_saliency_matrix: \",std_saliency_matrix)\n",
        "          print(\"std_saliency_matrix.shape: \",std_saliency_matrix.shape)\n",
        "        std_saliency_matrix_flatten = torch.flatten(std_saliency_matrix)\n",
        "        if debug:\n",
        "          print(\"mean std:\",np.round(torch.mean(std_saliency_matrix_flatten).item(),3))\n",
        "          print(\"mean std 5:\",np.round(torch.mean(torch.topk(std_saliency_matrix_flatten,5)[0]).item(),3))\n",
        "          print(\"mean std 10:\",np.round(torch.mean(torch.topk(std_saliency_matrix_flatten,10)[0]).item(),3))\n",
        "        json_data[\"mean_std_image_path_candidate_\"+str(index)] =  np.round(torch.mean(std_saliency_matrix_flatten).item(),3)\n",
        "        std_string=\"\"\n",
        "        for i in range(1,50):\n",
        "          if i>1:\n",
        "            std_string=std_string+\",\"+str(np.round(torch.mean(torch.topk(std_saliency_matrix_flatten,i)[0]).item(),3))\n",
        "          else:\n",
        "            std_string=str(np.round(torch.mean(torch.topk(std_saliency_matrix_flatten,i)[0]).item(),3))\n",
        "        json_data[\"std_average_csv_for_candidate_\"+str(index)] = std_string\n",
        "      else:\n",
        "        if debug:\n",
        "          print(\"Only one saliency map found for the label,\",index,\":\",idx_to_label(index),\" no mean or std can be calculated.\")\n",
        "      if debug:\n",
        "        print(json.dumps(json_data, indent=2))\n",
        "  return json_data\n"
      ],
      "metadata": {
        "id": "H9aTUv-YdyBP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run experiments!"
      ],
      "metadata": {
        "id": "IVA57u4-JLqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "experiment_path = \"/content/testset/\"\n",
        "experiment_name= \"version07\"\n",
        "save_path = 'trust_1/'\n",
        "if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "run_experiment(experiment_models, experiment_weights, experiment_path, experiment_name, use_predifined_tranformation=False, save_path=save_path, debug=False)\n",
        "json_data={}\n",
        "json_data[\"description\"] = \"Five models\"\n",
        "json_data[\"image tranformation\"] =\"standard image transformation: resize256 centercrop 224 and imagenet transforms\"\n",
        "with open(save_path+experiment_name+\"/description.json\", \"w\") as outfile:\n",
        "    outfile.write(json.dumps(json_data))"
      ],
      "metadata": {
        "id": "JcgQ9_kxHQ2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_name = \"trust_1\"\n",
        "XAI_compress = \"/\"\n",
        "import shutil\n",
        "shutil.make_archive(\"/content/\"+zip_name, 'zip', \"/content/trust_1\")"
      ],
      "metadata": {
        "id": "6gFHL5fZp4I_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bf9bde95-ee07-4888-ab30-8fb933a08bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/trust_1.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning if needed\n",
        "! rm -r trust_1\n",
        "! rm -r testset\n",
        "! rm -r testshapes2\n",
        "! rm imagenet_classes.txt"
      ],
      "metadata": {
        "id": "BL1TP7kJj5kZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b54dd2e2-e3dc-4c1b-e7d4-33976313be22"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'IKR': No such file or directory\n",
            "rm: cannot remove 'testshapes2': No such file or directory\n",
            "rm: cannot remove 'imagenet_classes.txt': No such file or directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "9DGYVMqjE7oN",
        "wBPIOLysxaO3"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}